{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Used - DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The navigation problem consist of training an agent which can collect as much yellow bananas from an environment as possible. The state space of the environment is a vector of size 37, and action space is a vector of size 4. For each yellow banana collected agent gets a reward of +1 and for each blue banana agent gets a reward of -1.\n",
    "\n",
    "To solve this we trained a DQN (https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) with experience replay. DQN with experience replay has 2 steps:\n",
    "\n",
    "1. Sample Step - In this the we use a NN to smaple possible actions given a set of states. We store the experiences - (state, reward, action, next_state) into a replay buffer. \n",
    "2. Training Step - After predecided timsteps we sample the replay buffer to train our NN described in the first step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of the NN\n",
    "\n",
    "For the NN we used a simple Feed Forward Neural Network with Relu activation, below are the layers in order:\n",
    "\n",
    "\n",
    "    Input (size 37)\n",
    "    \n",
    "        |\n",
    "        \n",
    "Fully Connected(size 64)\n",
    "\n",
    "        |\n",
    "        \n",
    "      Relu\n",
    "      \n",
    "        |\n",
    "        \n",
    "Fully Connected (size 64)\n",
    "\n",
    "        |\n",
    "        \n",
    "      Relu\n",
    "      \n",
    "        |\n",
    "        \n",
    "    Output (size 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Following hyperparameters were used to train the DQN\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "\n",
    "GAMMA = 0.99            # discount factor\n",
    "\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "\n",
    "LR = 5e-4               # learning rate \n",
    "\n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "n_episodes = 2000       # maximum number of episodes\n",
    "\n",
    "eps_start = 1.0         # start value for epsilon\n",
    "\n",
    "eps_end = 0.01          # minimum value for epsilon \n",
    "\n",
    "eps_decay = 0.995       # epsilon decay\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "It was required that the agent collects an average reward of 13 over 100 iterations. Initial experiments were able to achieve that requirement in about 550 iterations, so I decided to increase the threshold to 15 and was able to achieve that average score in 718 iterations.\n",
    "\n",
    "![alt text](dqn.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several improvements suggested to the DQN + experience replay algorithm. I am planning to implement a double DQN, a dueling DQN and prioritized experience replay and Rainbow approach (combination of all of the methods mentioned before). These enhancements are shown to provide a stable learning curve, reduce the number of iterations required to achieve convergence and also increase the expected rewards recieved at the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
